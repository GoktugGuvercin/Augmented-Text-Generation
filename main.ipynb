{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCJ7jhIMXTA4"
      },
      "source": [
        "# Retrieval Augmented Generation Pipeline\n",
        "\n",
        "This is a small-scale large language modeling (LLM) project which focuses on building a Retrieval-Augmented Generation (RAG) pipeline. The primary goal is to provide richer, more relevant context for user queries. In that way, the hallucinations will be prevented, more accurate and up-to-date answers will be generated and LLM archtecture will be more specialized for specific topics.\n",
        "\n",
        "This RAG pipeline specializes in deep learning theory and applications. It retrieves information from a curated Markdown file containing key concepts and methodologies in this domain, hosted on my [GitHub repository](https://github.com/GoktugGuvercin/The-Theory-of-Deep-Learning). This specialization allows the model to function as a subject matter expert within the field of deep learning.\n",
        "\n",
        "When we look at the working structure of our RAG application, we would see that it operates in three consecutive steps, which makes it well-suited for LangChain. Nevertheless, this project intentionally uses LangGraph to offer greater control and visibility over the workflow, a choice that will be elaborated on later.\n",
        "\n",
        "1. *Indexing*: The process begins by loading the external data from the GitHub source. This content is then divided into smaller, manageable chunks. Each chunk is converted into a numerical vector (embedding) and stored in a vector database.\n",
        "\n",
        "2. *Retrieval*: This indexing enables efficient similarity searches, allowing us to retrieve the most relevant text passages for any given user query. A vector store is associated with an embedding model. When we pass a textual query to similarity search function of this vector store, its embedding is generated and its top similar chunks are retrieved.\n",
        "\n",
        "3. *Generation:* Retrieved chunks are re-organized and inserted into a prompt. This prompt also includes the given query, and passed to chat model.\n",
        "\n",
        "In this project, we will use embed and chat models provided by Cohere. To be able to access them, we need to open a Cohere account and create an API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk75-MAj0yrk",
        "outputId": "86720c2b-ec73-4110-8ac9-525de3fb1ab1"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph\n",
        "!pip install langchain\n",
        "!pip install langchain-cohere\n",
        "!pip install langchain_community\n",
        "!pip install langchain-text-splitters\n",
        "!pip install \"unstructured[md]\"\n",
        "!pip install \"cohere==5.15.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_YWqNBYuu4rS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "from langchain_cohere import CohereEmbeddings\n",
        "from langchain_core.vectorstores import InMemoryVectorStore\n",
        "\n",
        "from langchain import hub\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "from IPython.display import Image, display\n",
        "from typing_extensions import TypedDict, List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGxK-mYnXQfE"
      },
      "source": [
        "# Cohere API Key\n",
        "\n",
        "Environment variables are a way to store configuration details (like API keys) outside of your actual code. Here, we check whether we have an API key for Cohere. If it is not, we enter and set our API key for model accessibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o41AC1ZxJT1W",
        "outputId": "86510377-3734-46b0-829a-c348052de93e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Cohere API key: ··········\n"
          ]
        }
      ],
      "source": [
        "if not os.getenv(\"COHERE_API_KEY\"):\n",
        "    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter your Cohere API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seDj3dLgRBih"
      },
      "source": [
        "# Indexing\n",
        "\n",
        "* ***Load:*** This is the first step of indexing. We load our external data sources by [*\\\"Document Loaders\\\"*](https://python.langchain.com/docs/concepts/document_loaders/). The data can be maintained in pdf, docx, structured sql table, or a web page; how you maintain your data only affects what kind of loader you will use.\n",
        "\n",
        "* ***Split:*** Our data can be quite comprehensive and considerably long. In this case, we use [*text-splitters*](https://python.langchain.com/docs/concepts/text_splitters/) to break into smaller chunks. It helps us to store the data in a more effective way and avoid some pitfalls that we can be encountered while searching them in vector databases, information retrieval, and answer generation steps.\n",
        "\n",
        "* ***Store:*** We store these chunks in [*vector databases*](https://python.langchain.com/docs/concepts/vectorstores/). At first, each chunk is indexed by an embedding vector to interpret its semantic context. Then, these chunks are loaded into the databases so that they can be searched over these embeddings. Embedding vectors are generated by [*representation models*](https://python.langchain.com/docs/concepts/embedding_models/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5lRLg6chGCG"
      },
      "source": [
        "### Loading Markdown Files\n",
        "\n",
        "Compared to PDF and DOCX files, markdown is considerably less preferred approach to maintain text, paragraphs and passages. Nevertheless, LangChain is capable of providing distinct document loaders for any kind of document structure.\n",
        "\n",
        "When markdown files are available on our local file system within the project workspace, the loader that we should use is [UnstructuredMarkdownLoader](https://python.langchain.com/docs/how_to/document_loader_markdown/). It is specifically designed to parse markdown syntax, and thereby helping to preserve the document's structure. To be able to use it, we need to install the necessary packages `nltk` and `\"unstructured[md]\"`. In contrast, accessing markdown files from web URLs, particularly from the platforms like GitHub, requires a slightly different approach. At this point, we opt for ***UnstructuredURLLoader***.\n",
        "\n",
        "Unstructured URL loaders are designed to handle different types of contents that target URL points to. How the data is maintained is not important, it can cope with any type including HTML, PDF, Docx and Markdown. This loader sends a HTTP-GET request to each URL given as input, hands the raw bytes off to *\\\"unstructured partitioning pipeline\\\"*, and that pipeline will auto-detect the type of content, which helps to call the right parser. A comprehensive guide to all URL loaders is available in [here](https://python.langchain.com/docs/integrations/document_loaders/url/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TBJH4RxikU4T"
      },
      "outputs": [],
      "source": [
        "markdown_url = \"https://raw.githubusercontent.com/GoktugGuvercin/The-Theory-of-Deep-Learning/refs/heads/main/README.md\"\n",
        "loader = UnstructuredURLLoader(urls=[markdown_url])\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqUlfvLRQ1Cj"
      },
      "source": [
        "### Split Documents into Chunks\n",
        "\n",
        "LLM architectures are restricted with context window; the number of tokens that they can process is limited. That is why, long documents are split into smaller pieces called *chunks*. Additionally, these smaller, but coherent passages support retrieval granularity. In other words, the passages are more aligned with the information that answers our query. If these passages are configured to be too long, they may convert into a chapter that only tangentially mentions the topic.\n",
        "\n",
        "The size of chunks has also important effect on their embedding quality. A single embedding vector for vast amount of text might average out the meaning, which makes it less effective at representing specific details and nuances within the chapter, which is called *dilution*. On the other hand, in too short chunks, it is highly possible to lack sufficient context for an embedding model to capture meaningful semantics. For more detailed explanation, you can check [this](https://github.com/GoktugGuvercin/The-Theory-of-Deep-Learning?tab=readme-ov-file#why-do-we-split-the-documents-into-smaller-chunks-for-vector-databases-).\n",
        "\n",
        "How we will approch this problem and what kind of methodology we will use to split the text are the key consideration. At this point, since we have specific markdown structure, we will use `MarkdownHeaderTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9A1XbvZiQ2ko"
      },
      "outputs": [],
      "source": [
        "splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=[(\"#\", \"Header 1\"), (\"##\", \"Header 2\")],\n",
        "    strip_headers=False,\n",
        ")\n",
        "\n",
        "chunks = splitter.split_text(documents[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqvZYBihlUaT"
      },
      "source": [
        "### Embeddings Models and Vector Datastores\n",
        "\n",
        "<ins>***Embedding Models:***</ins>\n",
        "\n",
        "Embedding models are used for generating dense vector representations to capture the context and semantic meaning of text passages, sentences, or documents. By calculating the distance or similarity between these vectors, we can quantitatively measure how related different pieces of text are. This capability is fundamental for applications like *semantic search*, which finds documents relevant to a query based on meaning rather than just keyword matching, and *recommender systems*, which suggest items similar to those a user has previously interacted with.\n",
        "\n",
        "Since embedding vectors are the representative of text passages in terms of  semantic meaning, we can use them to group or cluster these passages with respect to their topic or context. This is actually a typical example of unsupervised learning paradigm where *clustering algorithms* can be used. Besides, for *sentiment analysis* and *text classification* tasks, these embeddings can be possibly used. For example, qwen3 embedding model and its training strategy are tailored for asymmetrical semantic search; that is why, it achieves the best results in re-ranking and retrieval tasks of MTEB table, but small performance decrease happens in summarization, classification and clustering sections.\n",
        "\n",
        "<ins>***Vector Datastores:***</ins>\n",
        "\n",
        "Vectorstores are specialized database systems designed to store embedding vectors, which act as dense numerical fingerprints representing a piece of data, such as text passages, audio records, or images. Their main functionality is not only to store these embeddings along with original data, but also to provide the ability to index them efficiently and query them for information retrieval and similarity search. From these aspects, vectorstores become an important asset for semantic search engines, retrieval augmented generation (RAG) applications with LLMs, and also recommendation systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3pmwda32hJBe"
      },
      "outputs": [],
      "source": [
        "embedding = CohereEmbeddings(\n",
        "    cohere_api_key=os.environ[\"COHERE_API_KEY\"],\n",
        "    model=\"embed-v4.0\"\n",
        ")\n",
        "\n",
        "vectorstore = InMemoryVectorStore(embedding)\n",
        "chunk_ids = vectorstore.add_documents(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TB2Qjbfge01"
      },
      "source": [
        "# Retrieval and Generation\n",
        "\n",
        "This section outlines the design of the retrieval and augmentation logic. The process initiates when a user submits a question, which serves as the query. This query is at first transformed into an embedding vector to be searched on in-memory vectorstore. As a result of this search, the documents semantically related to the user's question are retrieved. Finally, these retrieved documents, along with the original question, are combined in a prompt to be passed to out LLM model for the generation of an answer.\n",
        "\n",
        "[LangChain Prompt Hub](https://smith.langchain.com/hub/), accessed by `langchain.hub`, is a central repository where people can share, discover, and use high-quality, pre-built prompt templates. Here, we can find well-engineered prompts for specific tasks, each of which is actually specified by a unique identifier. The `hub` refers to central online repository where all those prompts are stored. The function `pull()` downloads specific prompt from the hub into our code. At this point, ***\\\"rlm/rag-prompt\\\"*** is used as unique identifier in which we are downloading *\\\"rag-prompt\\\"* created by *\\\"rlm\\\"* user.\n",
        "\n",
        "```\n",
        "You are an assistant for question-answering tasks. Use the following pieces of\n",
        "retrieved context to answer the question. If you don't know the answer, just\n",
        "say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "```\n",
        "\n",
        "When we print the downloaded prompt, we see the one presented above. This prompt consists of multiple sub-sections that we try to benefit from so as to control how to proceed the conversation with LLM model. Let's investigate each of them:\n",
        "\n",
        "1. ***Setting the Persona:***\n",
        "    - *Instruction:* \"You are an assistant for question-answering tasks.\"\n",
        "    - *Purpose:* This tells the model what kind of job it is supposed to do. It sets the context for the entire mutual interaction, ensuring it behaves like a helpful assistant rather than, for example, a creative writer or a chatbot trying to have a casual conversation.\n",
        "\n",
        "2. ***Defining Core Rule:***\n",
        "    - *Instruction:* \"Use the following pieces of retrieved context to answer the question.\"\n",
        "    - *Purpose:* This command defines how the model answer the given questions. Instead of using its own internal (and potentially outdated or incorrect) knowledge, the model is directed to base its answer on the information provided in `context` section. This process is called grounding the model in your data.\n",
        "\n",
        "3. ***Escape Hatch:***\n",
        "    - *Instruction:* \"If you don't know the answer, just say that you don't know.\"\n",
        "    - *Purpose:* This is a specific safety guardrail. It tells the model how to behave if the provided context doesn't contain the answer. Without this, our LLM architecture can try to guess or hallucinate an answer, leading to incorrect information.\n",
        "\n",
        "4. ***Regulating Output:***\n",
        "    - *Instruction:* \"Use three sentences maximum and keep the answer concise.\"\n",
        "    - *Purpose:* This controls the format and length of the response.\n",
        "\n",
        "\n",
        "The sections `question` and `context` will be replaced by our inputs, while answer block will be completed by the response that the model will generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGIvohA8gic2",
        "outputId": "a88fbadc-3b52-40f7-f510-ac2d5909a866"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: my question \\nContext: retrieved documents \\nAnswer:\", additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "# our llm model\n",
        "llm = init_chat_model(\"command-a-03-2025\", model_provider=\"cohere\")\n",
        "\n",
        "# our target prompt to be filled with context and query\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "question = \"my question\"\n",
        "context = \"retrieved documents\"\n",
        "\n",
        "example_message = prompt.invoke(\n",
        "    {\"context\": f\"{context}\",\n",
        "     \"question\": f\"{question}\"}\n",
        ").to_messages()\n",
        "\n",
        "print(example_message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8jzHiHKl07p"
      },
      "source": [
        "# LangGraph\n",
        "\n",
        "We will use LangGraph to implement and develop our application logic, where we bring retrieval ang generation steps together to construct a robust connection between them. At this point, we need to define and determine 3 components:\n",
        "\n",
        "1. The state of application\n",
        "2. The nodes of application (actions or application steps)\n",
        "3. The edges and control flow of application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frwPFY9bdnuJ"
      },
      "source": [
        "## 1. State:\n",
        "\n",
        "The definition of the state relies on the type and scope of the application. However, its main ingredients commonly comprise <ins>***the input given to the application***</ins>, <ins>***the data processed and transferred between application steps***</ins>, and <ins>***generated output***</ins>. In this case, we keep track of input question, retrieved context, and generated answer in state variable.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GUSi2l7UdpCy"
      },
      "outputs": [],
      "source": [
        "class State(TypedDict):\n",
        "    question: str\n",
        "    context: List[Document]\n",
        "    answer: str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wz3lJQbfQbG"
      },
      "source": [
        "## 2. Nodes:\n",
        "\n",
        "We have only 2 application steps, which are ***retrieve*** and ***generation***. These actions are proceeeded sequentially. That is why, our application logic can be also implemented as a chain rather than a graph.\n",
        "\n",
        "* ***Retrieve:*** By using our input question, we perform similarity search on vector store. It returns the most related chunks as a list of documents. This step refers to *context retrieval*.\n",
        "\n",
        "* ***Generate:*** The retrieved context is a list of chunks. To put them in a usable format, we combine all those chunks in a string, each of which is separated from another one by new lines. Now, our context and question are ready to be included in message prompt. This prompt is passed to our chat model to generate a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HuTqNGiqfY1n"
      },
      "outputs": [],
      "source": [
        "def retrieve(state: State) -> dict:\n",
        "    retrieved_docs = vectorstore.similarity_search(state[\"question\"])\n",
        "    return {\"context\": retrieved_docs}\n",
        "\n",
        "\n",
        "def generate(state: State) -> dict:\n",
        "    docs_content = \"\\n\\n\".join(chunk.page_content for chunk in state[\"context\"])\n",
        "    input_fields = {\"question\": state[\"question\"],\"context\": docs_content}\n",
        "    message = prompt.invoke(input_fields)\n",
        "    response = llm.invoke(message)\n",
        "    return {\"answer\": response.content}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yng7sQo7tRnu"
      },
      "source": [
        "## 3. Edges and Control Flow\n",
        "\n",
        "Multiple nodes can be connected to one another in a sequential order. The `add_sequence()` function provides a convenient way to contruct this structure easily. It accepts a list of nodes as input, and the edges are constructed in the given order. `START` node is commonly used in LangGraph applications; it specifies where the invocation starts. In our application, it starts with the retrieval, so a virtual start node is instantiated and connected to `retrieve` node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "2-wIhFuOtVsg",
        "outputId": "6ee3daae-53d4-41dd-f914-b221f59082a4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
        "graph_builder.add_edge(START, \"retrieve\")\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPxaS8oB-Uw_",
        "outputId": "bb15536c-a5d8-4ed7-b0c6-3f9b0d59ee90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context: ## Why is weight sharing so important ?  \n",
            "Weight sharing enables us to reduce the number of trainable parameters in the neural architecture. This allows us to use more number of layers in order to learn more complicated relationships in the data, and the model even dont get larger enough.\n",
            "\n",
            "## Do we have weight-sharing in FC and convolution layers ?  \n",
            "Weight sharing is a concept of using same parameters to generate more than one output unit.  \n",
            "In a fully-connected layer, each neuron has its own weights. In that case, a different set of weights is used to produce each output unit. On the other hand, a convolution layer convolves same kernel values with different parts of the input to generate multiple units in output feature map. So, more than one output unit relies on same kernel values.  \n",
            "Each output unit relies on different set of parameters (FC Layer  \n",
            "No Weight Sharing).  \n",
            "Multiple output units relies on same set of parameters (Conv Layer  \n",
            "Weight Sharing).\n",
            "\n",
            "## What are conceptual idioms to describe there is no weight sharing in FC layers ?  \n",
            "Each neuron to generate an output unit has its own weight vectors.  \n",
            "Each element in weight matrix is used/visited only once to compute layer output.\n",
            "\n",
            "## Which factors decrease the number of parameters in CNNs ?  \n",
            "Sparse connectivity, ssing smaller kernels than input map. Even though input gets larger, kernel size can remain same (This is not the case for FC layers).  \n",
            "Weight sharing enables us to use same kernel values to generate multiple output units. So, each output unit does not rely on a new set of parameters.\n",
            "\n",
            "\n",
            "Answer: Weight sharing in deep learning reduces the number of trainable parameters by using the same set of weights to generate multiple output units, which is a key feature in convolutional layers but not in fully-connected layers. This allows for deeper models without significantly increasing model size, enabling the learning of more complex data relationships. In fully-connected layers, each neuron has its own weights, resulting in no weight sharing.\n"
          ]
        }
      ],
      "source": [
        "result = graph.invoke({\"question\": \"What do you know about weight sharing in deep learning?\"})\n",
        "context = \"\\n\\n\".join([doc.page_content for doc in result[\"context\"]])\n",
        "\n",
        "print(f'Context: {context}\\n\\n')\n",
        "print(f'Answer: {result[\"answer\"]}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
